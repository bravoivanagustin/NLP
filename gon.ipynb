{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8cf54f-c6e1-4397-831f-6315f1c942ab",
   "metadata": {},
   "source": [
    "# Ejemplo GPT Finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0251ff00-a149-422d-b39b-1d8a12cdd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.55.0 datasets peft trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995023f0-c73d-42e7-9e20-b4440195b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18205e39-d336-4a06-beaa-7854eff23746",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9210132d-699b-436d-8ae4-d46f27f24236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>USER USER USER i do not get this .. obviously ...</td>\n",
       "      <td>['a minor child deserves privacy and should be...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>USER USER trying to protest about . talking ab...</td>\n",
       "      <td>['USER USER why is he a loser ? he is just a p...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>USER USER USER he makes an insane about of mon...</td>\n",
       "      <td>['donald j . trump is guilty as charged . the ...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>USER USER meanwhile trump will not even releas...</td>\n",
       "      <td>['jamie raskin tanked doug collins . collins l...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>USER USER pretty sure the anti lincoln crowd c...</td>\n",
       "      <td>['man ... y all gone both sides the apocalypse...</td>\n",
       "      <td>twitter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           response  \\\n",
       "0      1  USER USER USER i do not get this .. obviously ...   \n",
       "1      1  USER USER trying to protest about . talking ab...   \n",
       "2      1  USER USER USER he makes an insane about of mon...   \n",
       "3      1  USER USER meanwhile trump will not even releas...   \n",
       "4      1  USER USER pretty sure the anti lincoln crowd c...   \n",
       "\n",
       "                                             context   source  \n",
       "0  ['a minor child deserves privacy and should be...  twitter  \n",
       "1  ['USER USER why is he a loser ? he is just a p...  twitter  \n",
       "2  ['donald j . trump is guilty as charged . the ...  twitter  \n",
       "3  ['jamie raskin tanked doug collins . collins l...  twitter  \n",
       "4  ['man ... y all gone both sides the apocalypse...  twitter  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/twitter_reddit/sarcasm_2_twitter.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9223a6b4-e824-43dc-8e56-8981e3c47559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER USER USER i do not get this .. obviously you do care or you would have moved right along .. instead you decided to care and troll her ..'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18a67bea-02ed-421a-86a9-5e1d622564d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['a minor child deserves privacy and should be kept out of politics . pamela karlan , you should be ashamed of your very angry and obviously biased public pandering , and using a child to do it .'\\n 'USER if your child is not named barron ... bebest melania could not care less . fact .']\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5b41b-c239-46e2-9311-60adb9cd167d",
   "metadata": {},
   "source": [
    "Dado que está todo en diferentes formatos, lo primero es hacer un poco de limpieza y conseguir un formato obicuo entre todos los datasets:\n",
    "- text: el texto sarcástico\n",
    "- is_sarcastic: booleano\n",
    "- degree_of_sarcasm: un entero del 0 al 10 (puede ser nulo)\n",
    "- paraphrase: el texto parafraseado sin sarcasmo si es que corresponde\n",
    "- context: el contexto del que proviene si es que es una respuesta a un hilo\n",
    "- type: one of \"sarcasm\", \"satire\", \"rhetorical question\", etc\n",
    "- source: la fuente, de que dataset proviene\n",
    "- task: la tarea para la cual se va a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "197748ca-8f02-49b4-9793-1c2852479e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, is_assistant=False):\n",
    "    count = text.count('USER')    \n",
    "    to_replace = ' '.join(['USER'] * count)\n",
    "\n",
    "    tag = \"\"\n",
    "    if is_assistant:\n",
    "        tag = f\"<|assistant|>\"\n",
    "    else:\n",
    "        tag = f\"<|user: {count}|>\"\n",
    "\n",
    "    if count == 0:\n",
    "        return ' '.join([tag, text])\n",
    "    else:\n",
    "        return text.replace(to_replace, tag)\n",
    "\n",
    "def process_context(context):\n",
    "    arr = context.replace('[', '').replace(']', '').replace(\"\\'\", '').split('\\n')\n",
    "    \n",
    "    comments_processed = []\n",
    "    for comment in arr:\n",
    "        processed = process_text(comment)\n",
    "        comments_processed.append(processed)\n",
    "\n",
    "    return \"\\n\".join(comments_processed)\n",
    "\n",
    "def new_row(df, index, source=''):\n",
    "    return {\n",
    "        'text': process_text(df.loc[index]['response'], is_assistant=True),\n",
    "        'is_sarcastic': df.loc[index]['label'],\n",
    "        'degree_of_sarcasm': None,\n",
    "        'paraphrase': None,\n",
    "        'context': process_context(df.loc[index]['context']),\n",
    "        'type': 'unknown',\n",
    "        'source': source,\n",
    "        'task': 'conversational'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52536bf1-6133-44a3-8a31-6b8411da7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de twitter formateado\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "for index, row in df.iterrows():\n",
    "    processed.append(new_row(df, index, source='twitter_reddit/sarcasm_2_twitter'))\n",
    "\n",
    "print(\"Dataset de twitter formateado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a5326-92a4-42e0-bcf4-28d4f7c5c943",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f8de486-6c8e-4bcc-ae58-075cd620e83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>yeah i mean there is only one gender anyways, ...</td>\n",
       "      <td>['lpt if you are worried about hurting someone...</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sounds like you do not like science, you theis...</td>\n",
       "      <td>['promotional images for some guy s facebook p...</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>of course play them in try mode, blizzard were...</td>\n",
       "      <td>['my friends will not play dota2 i will not pl...</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i do not understand, reddit told me that hilla...</td>\n",
       "      <td>['poll convention boosts clinton to 11 point l...</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>yeh, they are the reigning triple premiers, wh...</td>\n",
       "      <td>['wayne ludbey jordan lewis has the ultimate c...</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           response  \\\n",
       "0      1  yeah i mean there is only one gender anyways, ...   \n",
       "1      1  sounds like you do not like science, you theis...   \n",
       "2      1  of course play them in try mode, blizzard were...   \n",
       "3      1  i do not understand, reddit told me that hilla...   \n",
       "4      1  yeh, they are the reigning triple premiers, wh...   \n",
       "\n",
       "                                             context  source  \n",
       "0  ['lpt if you are worried about hurting someone...  reddit  \n",
       "1  ['promotional images for some guy s facebook p...  reddit  \n",
       "2  ['my friends will not play dota2 i will not pl...  reddit  \n",
       "3  ['poll convention boosts clinton to 11 point l...  reddit  \n",
       "4  ['wayne ludbey jordan lewis has the ultimate c...  reddit  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/twitter_reddit/sarcasm_2_reddit.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd91718b-1d6e-47bb-a248-65e3aa83fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user: 0|> poll convention boosts clinton to 11 point lead over trump in pa.\n",
      "<|user: 1|> 11 in pa 3 in az 15 in nh 9 in mi 1 in mo 1 in nv\n",
      "<|assistant|> i do not understand, reddit told me that hillary got a negative convention bump and that trump had nowhere to go but up.\n"
     ]
    }
   ],
   "source": [
    "# Solamente redefinimos process_context\n",
    "\n",
    "def process_context(context):\n",
    "    arr = context.replace('[', '').replace(']', '').replace(\"\\'\", '').split('\\n')\n",
    "    \n",
    "    comments_processed = []\n",
    "    for i, comment in enumerate(arr):\n",
    "        comment = ' '.join(['USER']*i) + comment\n",
    "        processed = process_text(comment)\n",
    "        comments_processed.append(processed)\n",
    "\n",
    "    return \"\\n\".join(comments_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "294bff32-fed1-40f9-8ed7-b012028cf18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset de reddit formateado\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "for index, row in df.iterrows():\n",
    "    processed.append(new_row(df, index, source='twitter_reddit/sarcasm_2_reddit'))\n",
    "\n",
    "print(\"Dataset de reddit formateado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d22e1f-ff82-4250-888c-26a8d54f863d",
   "metadata": {},
   "source": [
    "## Guardamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55421f9f-ea91-4483-9108-e5c13e340740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df_processed = pd.DataFrame(processed)\n",
    "os.makedirs(\"processed\", exist_ok=True)\n",
    "df_processed.to_csv(\"processed/gon.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f66cf-7c44-49ae-83c8-7e5aeb88ed9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638d35b-44b9-4877-8a94-e1042c1f491c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe22ae3-b4ff-491a-a832-8bd0349e383f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ddfcf-2718-4c1e-b6b4-8edd28f01815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c050d08-6979-469b-ae51-32b99bf1d268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', '__index_level_0__'],\n",
       "        num_rows: 1980\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', '__index_level_0__'],\n",
       "        num_rows: 220\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df[['text']])\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "569cc8c1-2f2f-46d5-852b-6c45a5fee7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo preentrenado\n",
    "model_name = \"gpt2\"  # o \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "special_tokens = {\"additional_special_tokens\": [\"<|commenter|>\", \"<|assistant|>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e94676-43f6-476a-95ac-065efd6370a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50258"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos el token especial\n",
    "tokenizer.convert_tokens_to_ids(\"<|assistant|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9afef2-9901-4ecb-ae8c-02f9bdfad9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52386644d52146b48bda45e9c25dead2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5a1210e1c948a0a793c7666ac09a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/220 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mask_user_part(example):\n",
    "    text = example[\"text\"]\n",
    "    max_len = 1024 \n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokens = tokenizer(\n",
    "        text, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=max_len,\n",
    "        return_tensors=None  # Asegurar que devuelve listas, no tensores\n",
    "    ) \n",
    "    \n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "    assistant_token_id = tokenizer.convert_tokens_to_ids(\"<|assistant|>\")\n",
    "\n",
    "    if assistant_token_id in input_ids:\n",
    "        start_idx = input_ids.index(assistant_token_id) + 1\n",
    "        labels[:start_idx] = [-100] * start_idx\n",
    "    else:\n",
    "        labels = [-100] * len(input_ids)\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "tokenized_datasets = dataset.map(mask_user_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8fbec6d-5f91-4b5b-b21e-052c8f28a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset:\n",
      "Longitud de input_ids: 1024\n",
      "Longitud de labels: 1024\n",
      "Número de elementos -100 en labels: 36\n"
     ]
    }
   ],
   "source": [
    "# Verificar un ejemplo del dataset\n",
    "print(\"Forma del dataset:\")\n",
    "print(f\"Longitud de input_ids: {len(tokenized_datasets['train'][0]['input_ids'])}\")\n",
    "print(f\"Longitud de labels: {len(tokenized_datasets['train'][0]['labels'])}\")\n",
    "\n",
    "# Verificar si hay -100 en los labels (partes ignoradas)\n",
    "labels_sample = tokenized_datasets['train'][0]['labels']\n",
    "print(f\"Número de elementos -100 en labels: {sum(1 for x in labels_sample if x == -100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ac29ea-214c-48be-b63a-519ba44f6ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/var/folders/2y/j5ctrp_92fn0v_b3_lr4c05w0000gn/T/ipykernel_55598/1946232105.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/gonzalospina/projects/nlp/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3960' max='3960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3960/3960 12:46:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.073019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.071992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.075763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalospina/projects/nlp/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/gonzalospina/projects/nlp/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/gonzalospina/projects/nlp/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/gonzalospina/projects/nlp/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./results/gpt2-gon/tokenizer_config.json',\n",
       " './results/gpt2-gon/special_tokens_map.json',\n",
       " './results/gpt2-gon/vocab.json',\n",
       " './results/gpt2-gon/merges.txt',\n",
       " './results/gpt2-gon/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.empty_cache()\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "model_output_dir = './results/gpt2-gon'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_output_dir)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_output_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d466f59b-491a-48bd-a57f-c126f0268619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|commenter|>', '<|assistant|>']\n",
      "Total number of paramerers: 124441344\n",
      "<|commenter|>  Terrific, that's a well poured beer \n",
      "  <|assistant|>  but i drink it once a week, so it must be good<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<|commenter|> Terrific, that's a well poured beer \\n <|assistant|>\"\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "model_path = './results/gpt2-gon'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "print(tokenizer.additional_special_tokens)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Calculate the Number of Parameters in the model being used for inference\n",
    "total_params = get_model_parameters(model)\n",
    "print(f\"Total number of paramerers: {total_params}\")\n",
    "\n",
    "# Prepare the input text you want to generate predictions for\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate Text\n",
    "outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35bd85-1ede-4b40-9627-5ca1196cf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"datasets/sarc/sarc.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83748e73-a0a0-4e8b-bb60-da4acf0153c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
